# 数据采集scrape模块
无论是静态文件 还是通过服务发现，我们都已经解决**监控谁**的问题.那么下面就要拉取监控指标了。在`Prometheus`中由`Scrape.Manager`(定义文件`scrape/manager.go`) 管理监控对象。

##  Scrape.Manager 定义
文件: `scrape/manager.go` 
```
// Manager maintains a set of scrape pools and manages start/stop cycles
// when receiving new target groups from the discovery manager.
type Manager struct {
	opts      *Options
	logger    log.Logger
	append    storage.Appendable
	graceShut chan struct{}

	offsetSeed    uint64     // Global offsetSeed seed is used to spread scrape workload across HA setup.
	mtxScrape     sync.Mutex // Guards the fields below.
	scrapeConfigs map[string]*config.ScrapeConfig
	scrapePools   map[string]*scrapePool
	targetSets    map[string][]*targetgroup.Group  // 要拉取的target
	buffers       *pool.Pool

	triggerReload chan struct{}  // reload信号

	metrics *scrapeMetrics
}
```


## 监控对象

根据配置文件`prometheus.yml`中关于监控对象的配置，如下： 
```
scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]
  - job_name: "job-0"
    metrics_path: '/metrics'
    scheme : 'http'
    static_configs:
      - targets: ["127.0.0.1:8520","192.168.0.103:8520"]
```
可以看出：
- 所有的监控对象是按照监控对象进行分组的，每个组都有一个`job_name`,例如：`job_name: "prometheus"`,`job_name: "job-0"`
- 每个监控对象的`targets`有多个具体的`endpoint`，例如 `job-0`的 `targets:["127.0.0.1:8520","192.168.0.103:8520"]`


`Scrape.Manager.targetSets` 就是管理这些**监控对象**的数据结构,类型：`map[string][]*targetgroup.Group`
- 这个`map`的`key`就是配置中的`job_name`
- 这个`map`的`value`类型：`[]*targetgroup.Group`。

```
{
    "prometheus": [
        {
            "Targets": [
                {
                    "__address__": "localhost:9090"
                }
            ],
            "Labels": null,
            "Source": "0"
        }
    ],
    "job-0": [
        {
            "Targets": [
                {
                    "__address__": "127.0.0.1:8520"
                },
                {
                    "__address__": "192.168.0.103:8520"
                }
            ],
            "Labels": null,
            "Source": "0"
        }
    ]
}

```


## `Scrape Manager`入口

`Scrape Manager`(*注：`cmd/prometheus/main.go`*)入口代码如下：  
```
{
		// Scrape manager.
		g.Add(
			func() error {
				// When the scrape manager receives a new targets list
				// it needs to read a valid config for each job.
				// It depends on the config being in sync with the discovery manager so
				// we wait until the config is fully loaded.
				<-reloadReady.C

				err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
				level.Info(logger).Log("msg", "Scrape manager stopped")
				return err
			},
			func(err error) {
				// Scrape manager needs to be stopped before closing the local TSDB
				// so that it doesn't try to write samples to a closed storage.
				// We should also wait for rule manager to be fully stopped to ensure
				// we don't trigger any false positive alerts for rules using absent().
				level.Info(logger).Log("msg", "Stopping scrape manager...")
				scrapeManager.Stop()
			},
		)
	}
```
说明：
- **`err := scrapeManager.Run(discoveryManagerScrape.SyncCh())`** 为 `Scrape Manager` 启动函数。定义为`func (m *Manager) Run(tsets <-chan map[string][]*targetgroup.Group) error`


## 