# 数据采集scrape模块(todo)
无论是静态文件 还是通过服务发现，我们都已经解决**监控谁**的问题.那么下面就要拉取监控指标了。数据采集scrape模块主要有两方面的功能:
- **管理、更新待拉取的目标** : 通过服务发现，`Prometheus`总是会拿到当前最新的监控对象的信息(*例如：拉取`mertics`的地址等*)。`scrape.Manager`对象负责这些需要进行scrape的target
- **拉取mertics** : 发起`http(s)`请求,拉取监控指标




## 管理、更新待拉取的目标

在`Prometheus`中由`Scrape.Manager`(定义文件`scrape/manager.go`) 管理监控对象。

###  Scrape.Manager 定义  

文件: `scrape/manager.go` 
```
// Manager maintains a set of scrape pools and manages start/stop cycles
// when receiving new target groups from the discovery manager.
// scrape.Manager维护一组scrapePool,scrapePool负责拉取监控指标等工作
// 当通过 discover manager 获取到当前最新的抓取目标的时，scrape.Manager热更新最新的监控目标 并管理scrapePool循环的的启动、关闭  

type Manager struct {
	opts      *Options
	logger    log.Logger
	append    storage.Appendable                      // 存储
	graceShut chan struct{}                           // 关闭信号

	offsetSeed    uint64     
	mtxScrape     sync.Mutex 
	scrapeConfigs map[string]*config.ScrapeConfig    // prometheus.yml配置文件中scrape_configs模块信息: 拉取的target配置的初始值信息,key为job_name
	scrapePools   map[string]*scrapePool             // 存储了一组拉取指标的实际执行者
	targetSets    map[string][]*targetgroup.Group    // target更新要拉取的具体target,key为job_name
	buffers       *pool.Pool

	triggerReload chan struct{}                      // 传递reload信号的channel，通过监听此channel进行reload操作

	metrics *scrapeMetrics                           // 对scrape模块监控指标
}
```  

**主要字段**：
| 字段名   | 类型    |说明 | 
| :-----| :---- | :---- |
| `scrapeConfigs`  |`map[string]*config.ScrapeConfig` | `prometheus.yml`配置文件中`scrape_configs`部分的信息。`map`的`key`是`prometheus.yml`配置文件中的`job_name`,`map`的`value`是对应的配置内容  |
| `scrapePools`   |`map[string]*scrapePool` | 存储了一组拉取指标的实际执行者,`map`的`key`是`job_name`, `map`的`value`类型是`scrapePool` ,是拉取对应`job_name`指标的实际工作者。 |
| `targetSets`   |`map[string][]*targetgroup.Group` | 服务发现模块会将当前最新的监控对象封装成`map[string][]*targetgroup.Group`，通过`channel`(注：`channel`的类型`chan map[string][]*targetgroup.Group`)发送给`scrape.Manager`。 `scrape.Manager`会把接收到的信息暂存在`targetSets`字段。`map`的`key`是`job_name`,`map`的`value`就是对应的监控对象信息 |
| `triggerReload`  |`chan struct{}`  | 用于传递热更新信号，`scrape.Manager`将接收到的信息暂存在`targetSets`字段后，会向`triggerReload`发送更新信号。`scrape.Manager`的`reloader`方法接收到更新信号后，调用更新操作。 |

<br/>  
<br/>  

**以静态文件配置为例进行说明**  

demo:4.0.1: 配置文件`prometheus.yml`中`scrape_configs`部分的配置

```
scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]
  - job_name: "job-0"
    metrics_path: '/metrics'
    scheme : 'http'
    static_configs:
      - targets: ["127.0.0.1:8520","192.168.0.103:8520"]
```

可以看出：
- 所有的监控对象是按照监控对象进行分组的，每个组都有一个`job_name`,例如：`job_name: "prometheus"`,`job_name: "job-0"`
- 每个监控对象的`targets`有多个具体的`endpoint`，例如 `job-0`的 `targets:["127.0.0.1:8520","192.168.0.103:8520"]`


`prometheus`中`Scrape.Manager.targetSets` 就是管理这些**监控对象**的数据结构,类型：`map[string][]*targetgroup.Group`
- 这个`map`的`key`就是配置中的`job_name`
- 这个`map`的`value`类型：`[]*targetgroup.Group`。 `targetgroup.Group` 定义如下:  
```
type Group struct {
	Targets []model.LabelSet  // model.LabelSet 本质是map[string]string
	Labels model.LabelSet     // model.LabelSet 本质是map[string]string

	// Source is an identifier that describes a group of targets.
	Source string
}
```

`demo:4.0.1`对应`Scrape.Manager.targetSets`的数据结构： 

```
{
    "prometheus": [
        {
            "Targets": [
                {
                    "__address__": "localhost:9090"
                }
            ],
            "Labels": null,
            "Source": "0"
        }
    ],
    "job-0": [
        {
            "Targets": [
                {
                    "__address__": "127.0.0.1:8520"
                },
                {
                    "__address__": "192.168.0.103:8520"
                }
            ],
            "Labels": null,
            "Source": "0"
        }
    ]
}

```


## 更新target 

`Scrape Manager`执行过程

`Scrape Manager`(*注：`cmd/prometheus/main.go`*)入口代码如下：  
```
{
		// Scrape manager.
		g.Add(
			func() error {
				// When the scrape manager receives a new targets list
				// it needs to read a valid config for each job.
				// It depends on the config being in sync with the discovery manager so
				// we wait until the config is fully loaded.
				<-reloadReady.C

				err := scrapeManager.Run(discoveryManagerScrape.SyncCh())
				level.Info(logger).Log("msg", "Scrape manager stopped")
				return err
			},
			func(err error) {
				// Scrape manager needs to be stopped before closing the local TSDB
				// so that it doesn't try to write samples to a closed storage.
				// We should also wait for rule manager to be fully stopped to ensure
				// we don't trigger any false positive alerts for rules using absent().
				level.Info(logger).Log("msg", "Stopping scrape manager...")
				scrapeManager.Stop()
			},
		)
	}
```

说明： 
- `err := scrapeManager.Run(discoveryManagerScrape.SyncCh())`  传入的参数是`chan`，`send`端是`discovery`组件，将最新的`targets`传递给`scrape`组件，`scrape`组件更新拉取的目标地址等信息
  
<br/>  

监听target变化：  
```
// Run receives and saves target set updates and triggers the scraping loops reloading.
// Reloading happens in the background so that it doesn't block receiving targets updates.

func (m *Manager) Run(tsets <-chan map[string][]*targetgroup.Group) error {
	go m.reloader()
	// 循环
	for {
		select {
		case ts := <-tsets:   // 在chan tsets 获取到当前targets, chan tsets的send端一般是服务发现组件
			m.updateTsets(ts) // 更新targets,将 m.targetSets 设置为ts

			select {
			case m.triggerReload <- struct{}{}:  // 发生reload信号
			default:
			}

		case <-m.graceShut:
			return nil
		}
	}
}

func (m *Manager) updateTsets(tsets map[string][]*targetgroup.Group) {
	m.mtxScrape.Lock()
	m.targetSets = tsets
	m.mtxScrape.Unlock()
}

```

监听reload信号，加载target

```

func (m *Manager) reloader() {
	reloadIntervalDuration := m.opts.DiscoveryReloadInterval
	if reloadIntervalDuration < model.Duration(5*time.Second) {
		reloadIntervalDuration = model.Duration(5 * time.Second)
	}

	ticker := time.NewTicker(time.Duration(reloadIntervalDuration))

	defer ticker.Stop()

	for {
		select {
		case <-m.graceShut:
			return
		case <-ticker.C:
			select {
			case <-m.triggerReload: // 监听到reload信号，执行reload操作
				m.reload()          // 实际上加载targets的操作
			case <-m.graceShut:
				return
			}
		}
	}
}
``` 
  
说明：
- 通过 `chan tsets` 接受到获取到当前监控的`targets`,暂存在 `scrapeManager`对象的`targetSets`字段，以待更新，并且向 `scrapeManager`对象的`triggerReload`发送信号
- `reloader`方法定期(默认5秒)去尝试获取`scrapeManager`对象的`triggerReload`信号。接收到reload信号，则执行`m.reload()` 加载

<br/>  


Manager.reload-热加载的实际执行者
  
```

func (m *Manager) reload() {
	m.mtxScrape.Lock()
	var wg sync.WaitGroup
	// m.targetSets 暂存的是当前最新的抓取目标，是在 Manager.Run--> Manager.updateTsets(ts) 中进行设置的   
	// 遍历m.targetSets为每个job创建 scrapePool
	for setName, groups := range m.targetSets {
		// check 是否存在jop_name的scrapePools，如果不存在，则创建
		if _, ok := m.scrapePools[setName]; !ok {
			// 配置文件中是否存在此job
			scrapeConfig, ok := m.scrapeConfigs[setName]
			if !ok {
				level.Error(m.logger).Log("msg", "error reloading target set", "err", "invalid config id:"+setName)
				continue
			}
			// 为每个targetSet创建scrapePool实例
			m.metrics.targetScrapePools.Inc()  // 创建scrapePool实例,监控指标 +1 
			sp, err := newScrapePool(scrapeConfig, m.append, m.offsetSeed, log.With(m.logger, "scrape_pool", setName), m.buffers, m.opts, m.metrics)
			if err != nil {
				m.metrics.targetScrapePoolsFailed.Inc() // 创建失败,监控指标 +1 
				level.Error(m.logger).Log("msg", "error creating new scrape pool", "err", err, "scrape_pool", setName)
				continue
			}
			m.scrapePools[setName] = sp
		}

		// 启动协程，向scrapePool同步最新的Target Group
		// sp.Sync(groups)  将 Target Group 转换为实际的抓取目标Target，
		// 同步当前运行的 scraper 和结果集，返回全部抓取和丢弃的目标。
		wg.Add(1)
		// Run the sync in parallel as these take a while and at high load can't catch up.
		go func(sp *scrapePool, groups []*targetgroup.Group) {
			sp.Sync(groups)
			wg.Done()
		}(m.scrapePools[setName], groups)

	}
	m.mtxScrape.Unlock()
	wg.Wait()
}
```
说明：
-  **m.targetSets**     `m.targetSets ` 暂存的是当前最新的抓取目标，是在  `Manager.Run--> Manager.updateTsets(ts) ` 中进行设置的. 此处遍历 `m.targetSets ` 为每个 `job `创建 `scrapePool`实例
-  **m.metrics.targetScrapePools.Inc()**   `Manager.metrics` 是对`scrape`模块的监控指标， `Manager.metrics.targetScrapePools`是`Prometheus`的`scrapePool`实例计数，`counter`类型，`metric names`:`prometheus_target_scrape_pools_total`。创建一个新`scrapePool`实例，则`Manager.metrics.targetScrapePools  + 1` ；同理，创建失败则执行`m.metrics.targetScrapePoolsFailed.Inc()`
  
<br/>  


## 抓取指标

scrapePool 结构体

定义  
```

// scrapePool manages scrapes for sets of targets.
type scrapePool struct {
	appendable storage.Appendable       // 存储,此接口定义了存储的行为
	logger     log.Logger
	cancel     context.CancelFunc
	httpOpts   []config_util.HTTPClientOption

	// mtx must not be taken after targetMtx.
	mtx    sync.Mutex
	config *config.ScrapeConfig       // 抓取的配置
	client *http.Client               // http client,用于pull指标时 发起http请求
	loops  map[uint64]loop

	targetMtx sync.Mutex
	// activeTargets and loops must always be synchronized to have the same
	// set of hashes.
	activeTargets       map[uint64]*Target  // 抓取的目标endpoint等信息
	droppedTargets      []*Target // Subject to KeepDroppedTargets limit.
	droppedTargetsCount int       // Count of all dropped targets.

	// Constructor for new scrape loops. This is settable for testing convenience.
	newLoop func(scrapeLoopOptions) loop

	noDefaultPort bool

	metrics *scrapeMetrics       // 监控指标
}
```
主要字段说明： 
-  **`appendable storage.Appendable`** 存储`storage`，`scrapePool`实例`appendable`被赋值为 `fanoutStorage`(TODO：图片`main`-> `scrape.NewManager`;scrapeManager.reload->newScrapePool)
-  **`config *config.ScrapeConfig`**  抓取的配置
-  **`client *http.Client`**     http client,用于pull指标时 发起http请求
  
