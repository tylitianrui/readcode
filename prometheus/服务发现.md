# 服务发现
作为监控系统，`prometheus`首先要解决的就是"`要监控谁`"的问题。静态配置比较适合`targets`基本稳定不变的场景，云原生体系下所有的被监控target都在动态的变化，如果使用静态配置就相形见绌了(*注：在[001.prometheus简述](001.prometheus简述.md)章节已经做了简述*）。本章主要讨论**服务发现**。

最新版本的`prometheus`(`v2.53`) 支持39种服务发现协议。配置关键字如下：  
- azure_sd_config
- consul_sd_config
- digitalocean_sd_config
- docker_sd_config
- dockerswarm_sd_config
- dns_sd_config
- ec2_sd_config
- openstack_sd_config
- ovhcloud_sd_config
- puppetdb_sd_config
- file_sd_config
- gce_sd_config
- hetzner_sd_config
- http_sd_config
- ionos_sd_config
- **kubernetes_sd_config**
- kuma_sd_config
- lightsail_sd_config
- linode_sd_config
- marathon_sd_config
- nerve_sd_config
- nomad_sd_config
- serverset_sd_config
- triton_sd_config
- eureka_sd_config
- scaleway_sd_config
- uyuni_sd_config
- vultr_sd_config
- **static_config**

由上述服务发现配置关键字，可以发现：
- 服务发现的配置都是以 `xxxx_sd_config`。因为`_sd_config`后缀是在代码中`hard-code`写死的，所以二开新的服务发现协议，需要遵守此命名规则。
- **static_config** 是一种特殊的服务发现。可以理解为target始终不变的服务发现。也是最简单的服务发现形式  

<br/>
服务发现的配置详见: https://prometheus.io/docs/prometheus/2.50/configuration/configuration/  

<br/>  
<br/>  

本章分别介绍 最简单的**static_config**和最常用的**kubernetes_sd_config** 服务发现。其他的服务发现实现逻辑是相同的，不再讲述。  


<!-- ## prometheus监控外部kubernetes集群配置

静态配置不再赘述，可见[prometheus简述:静态文件配置](./prometheus简述.md#静态文件配置)

为了方便调试`prometheus`代码，`prometheus`在本地运行、debug，监控`kubernetes`集群。 针对`kubernetes`集群外部部署(或运行的)`prometheus` 首先需要创建`token` ,这样`prometheus`能够访问`kubernetes`集群的`apiserver`获取监控数据。

### 步骤1：创建`token`,即创建`prometheus`访问权限

#### 1.1 在`kubernetes`集群`master`节点上创建文件 [`rbac-setup.yaml`](https://github.com/prometheus/prometheus/blob/v2.53.0/documentation/examples/rbac-setup.yml) 
  
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
  - apiGroups: [""]
    resources:
      - nodes
      - nodes/metrics
      - services
      - endpoints
      - pods
    verbs: ["get", "list", "watch"]
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses
    verbs: ["get", "list", "watch"]
  - nonResourceURLs: ["/metrics", "/metrics/cadvisor"]
    verbs: ["get"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: default
---
apiVersion: v1
kind: Secret
metadata:
  name: prometheus-sa-token
  namespace: default
  annotations:
    kubernetes.io/service-account.name: prometheus
type: kubernetes.io/service-account-token
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: default

```

#### 1.2 `kubernetes`集群`master`节点上执行`kubectl  apply  -f  rbac-setup.yaml`  
  
```shell
$ kubectl  apply  -f rbac-setup.yaml

clusterrole.rbac.authorization.k8s.io/prometheus created
serviceaccount/prometheus created
secret/prometheus-sa-token created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
```

### 步骤2: 获取`token`

[步骤1](#步骤1创建token即创建prometheus访问权限)创建`rbac`时，是在`namespace:default`下进行的。作用可以直接省略 `-n default` 参数

#### 2.1  获取`token`名称 

`kubernetes`集群`master`节点上执行`kubectl get secret`

```shell
$  kubectl get secret

NAME                  TYPE                                  DATA   AGE
prometheus-sa-token   kubernetes.io/service-account-token   3      46s
```

`prometheus-sa-token` 就是刚才创建的`token`名称。
<br>

#### 2.2 获取`token`内容

 `kubernetes`集群`master`节点上,执行`kubectl  describe secrets  prometheus-sa-token` 
  
```shell
$ kubectl  describe secrets  prometheus-sa-token

Name:         prometheus-sa-token
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: prometheus
              kubernetes.io/service-account.uid: 516bf408-a6ba-4ec3-b242-9e46118951a8

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1107 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InhJaVZJS1g1aTgtZ2JGY0Z3dWNoR0FhV3hOMzVIX0J6NXdCY3RVbWM4MDgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InByb21ldGhldXMtc2EtdG9rZW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicHJvbWV0aGV1cyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjUxNmJmNDA4LWE2YmEtNGVjMy1iMjQyLTllNDYxMTg5NTFhOCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnByb21ldGhldXMifQ.eqNYzgSTUPqabbO-mEKHPMOGVkgLcmHFcuYgHjPS5nwFnf_1TBOJ_9roSGfs9RHE2JXPLj3t4e0lcoMRnX_i32oEbI2qSOoQ6L-2sZ2MGmYsWmSE6WtYyTFRxsIFfLNNcKxPwKgwXAxdo5QNpxDQ4VUjaMBdqeth2Z1uGXhN1tf295rH9e-DaoZbgY78_gh4GWwYvMv5F7gEP6O6a5oczbEApLwtPnunrZdQ2YeyNQYsSsfQSBko1iIdFm0TEXgZi2-Zp17Wz9UE8x0HYjwvR95P-rvCAh1x3WaTux6Mddm8xO9QtYBhht_gdWElWzkSQFY1yUYm0ts6PYRBOpmW7w
```
<br>

#### 2.3 将`token`保存本地文件内 

将`token`保存本地文件内，文件名字任意。作者将其保存在prometheus项目文件中`documentation/examples/k8s.token`(绝对路径:`/Users/tyltr/opencode/prometheus/documentation/examples/k8s.token`)

### 步骤3：获取`kubernetes`集群的`api server`的地址  

在`kubernetes`集群`master`节点上,执行`cat ~/.kube/config | grep server` 

```shell
$ cat ~/.kube/config | grep server

server: https://192.168.0.107:6443
```

注：`https://192.168.0.107:6443` 就是 `kubernetes`集群的`api server`的地址


### 步骤4: `prometheus`配置文件

`prometheus`配置文件,作者命名为`k8s-prometheus.yaml`,内容如下：

```yaml
global:
  keep_dropped_targets: 100

scrape_configs:
  - job_name: "kubernetes-apiservers"
    scheme: https
    kubernetes_sd_configs:
      - api_server: https://192.168.0.107:6443
        role: endpoints
        namespaces:
          names: ["default"]
        bearer_token_file: /Users/ollie/opencode/prometheus/documentation/examples/k8s.token
        tls_config:
          insecure_skip_verify: true
    bearer_token_file:   /Users/ollie/opencode/prometheus/documentation/examples/k8s.token
    tls_config:
      insecure_skip_verify: true
    relabel_configs:
      - source_labels:
          [
            __meta_kubernetes_namespace,
            __meta_kubernetes_service_name,
            __meta_kubernetes_endpoint_port_name,
          ]
        action: keep
        regex: default;kubernetes;https

# node
  - job_name: "kubernetes-nodes"
    scheme: https
    kubernetes_sd_configs:
      - api_server: https://192.168.0.107:6443
        role: node
        namespaces:
          names: ["default"]
        bearer_token_file: /Users/ollie/opencode/prometheus/documentation/examples/k8s.token
        tls_config:
          insecure_skip_verify: true
    bearer_token_file:   /Users/ollie/opencode/prometheus/documentation/examples/k8s.token
    tls_config:
      insecure_skip_verify: true
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

```

注：`bearer_token_file` 为 [2.3`token`存放的目录](#23-将token保存本地文件内)

### 步骤5：运行

```shell
./prometheus  --config.file=/your/path/k8s-prometheus.yml
```

即可在`web ui`上观察到`target`列表  

![k8s-demo](src/k8s-prometheus-demo.png)






 -->

<!-- ## 服务发现的核心逻辑   -->

`prometheus`的服务发现由`Discovery Manager`进行管理和维护的。`Discovery Manager`维护三种协程实现服务发现，每种协程负责不同的工作：

- 服务发现协程`discovery goroutine`
- 更新协程`updater goroutine`
- 发送协程`sender goroutine`

各协程示意图  

![discovery goroutine](src/服务发现的goroutine.svg "discovery core logic")  

### DiscoveryManager

`prometheus` 定义了很多`Manager`,例如负责服务发现的`DiscoveryManager`、负责拉取指标的`ScrapeManager`等。在源码中，`DiscoveryManager`被定义为名为[Manager的结构体](https://github.com/prometheus/prometheus/blob/v2.53.0/discovery/legacymanager/manager.go#L87)。
除此之外，还有[`provider`](https://github.com/prometheus/prometheus/blob/v2.53.0/discovery/legacymanager/manager.go#L37)也是比较重要的。它封装了各个服务发现的实例和配置。

DiscoveryManager  

```golang
// Manager maintains a set of discovery providers and sends each update to a map channel.
// Targets are grouped by the target set name.
type Manager struct {
	logger         log.Logger
	name           string
	mtx            sync.RWMutex
	ctx            context.Context
	discoverCancel []context.CancelFunc

	// Some Discoverers(eg. k8s) send only the updates for a given target group
	// so we use map[tg.Source]*targetgroup.Group to know which group to update.
	targets map[poolKey]map[string]*targetgroup.Group
	// providers keeps track of SD providers.
	providers []*provider
	// The sync channel sends the updates as a map where the key is the job value from the scrape config.
	syncCh chan map[string][]*targetgroup.Group

	// How long to wait before sending updates to the channel. The variable
	// should only be modified in unit tests.
	updatert time.Duration

	// The triggerSend channel signals to the manager that new updates have been received from providers.
	triggerSend chan struct{}

	// A registerer for all service discovery metrics.
	registerer prometheus.Registerer

	metrics   *discovery.Metrics
	sdMetrics map[string]discovery.DiscovererMetrics
}

```

重要字段说明：

- `targets map[poolKey]map[string]*targetgroup.Group`  存储了`prometheus`获取到的所有的服务发现信息
- `providers []*provider`  各个服务发现的实例 
- `syncCh chan map[string][]*targetgroup.Group`  向`scrape`模块发送服务发现结果
- `triggerSend chan struct{}`  通过此`channal`，`updater`协程向`sender`协程发送信号，通知服务发现有新更新。

私有结构体`provider` 定义：

```go
// provider holds a Discoverer instance, its configuration and its subscribers.
type provider struct {
	name   string
	d      discovery.Discoverer
	subs   []string
	config interface{}
}

```
重要字段：
- `d   discovery.Discoverer`  各种协议的服务发现，如果k8s，d就是[`kubernetes.Discovery`](https://github.com/prometheus/prometheus/blob/v2.53.0/discovery/kubernetes/kubernetes.go#L263)，



### discovery.Discoverer
TODO 


### 服务发现协程`discovery goroutine`

`discovery`协程主要工作就是进行**服务发现**,获取`targets`拉取`metric`的地址。实际环境中，被监控的target众多，每个target的协议各不相同。所以需要为每种协议创建不同的对象来负责具体的服务发现。 例如:`kubernetes_sd_configs`协议对应的是[`kubernetes.Discovery`](https://github.com/prometheus/prometheus/blob/v2.53.0/discovery/kubernetes/kubernetes.go#L263)；`ec2_sd_config`协议对应的是[`aws.EC2Discovery`](https://github.com/prometheus/prometheus/blob/v2.53.0/discovery/aws/ec2.go#L146)等，

虽然是不同对象负责不同协议的服务发现，但每种协议的服务发现对象必须都实现[`discovery.Discoverer`](https://github.com/prometheus/prometheus/blob/v2.53.0/discovery/discovery.go#L35)接口。 `discovery.Discoverer`定义如下:

```golang 
// Discoverer provides information about target groups. It maintains a set
// of sources from which TargetGroups can originate. Whenever a discovery provider
// detects a potential change, it sends the TargetGroup through its channel.
//
// Discoverer does not know if an actual change happened.
// It does guarantee that it sends the new TargetGroup whenever a change happens.
//
// Discoverers should initially send a full set of all discoverable TargetGroups.
type Discoverer interface {
	// Run hands a channel to the discovery provider (Consul, DNS, etc.) through which
	// it can send updated target groups. It must return when the context is canceled.
	// It should not close the update channel on returning.
	Run(ctx context.Context, up chan<- []*targetgroup.Group)
}
```
<br> 

`discovery.Discoverer`接口只有一个方法`Run(ctx context.Context, up chan<- []*targetgroup.Group)`。其中参数`up chan<- []*targetgroup.Group`是`discovery`协程与`updater`协程交互的"渠道"。`discovery`协程会将服务发现的结果封装成`[]*targetgroup.Group`发送到`updater`协程。

<br>

`prometheus`配置文件中可以配置多个服务发现，`prometheus`会为每个服务发现配置开启独立的`discovery`协程和`updater`协程。  
例如: 配置文件(`配置demo1`)中设置了3处服务发现的配置，`job_name: kubernetes-apiservers-jobnanme` 设置1个服务发现配置，`job_name: kubernetes-nodes-jobnanme`设置2个服务发现配置。那么`Discovery Manager`会创建3个独立的`kubernetes.Discovery`实例，并且每个`kubernetes.Discovery`实例都会开启一个独立`discovery`协程，进行服务发现。除此次之外，`Discovery Manager`也会开启独3个`updater`协程分别接收`discovery`协程获取的服务信息。

**配置demo1**  

```yaml 
global:
  keep_dropped_targets: 100

scrape_configs:
  - job_name: "kubernetes-apiservers-jobnanme"
    scheme: https
    kubernetes_sd_configs:
      - api_server: https://192.168.0.107:6443
        role: endpoints
        namespaces:
          names: ["default"]
        bearer_token_file: /Users/ollie/opencode/prometheus/documentation/examples/k8s.token
        tls_config:
          insecure_skip_verify: true
    bearer_token_file:   /Users/ollie/opencode/prometheus/documentation/examples/k8s.token
    tls_config:
      insecure_skip_verify: true
    relabel_configs:
      - source_labels:
          [
            __meta_kubernetes_namespace,
            __meta_kubernetes_service_name,
            __meta_kubernetes_endpoint_port_name,
          ]
        action: keep
        regex: default;kubernetes;https

  - job_name: "kubernetes-nodes-jobnanme"
    scheme: https
    kubernetes_sd_configs:
      - api_server: https://192.168.0.107:6443
        role: node
        namespaces:
          names: ["default"]
        bearer_token_file: /Users/ollie/opencode/prometheus/documentation/examples/k8s.token
        tls_config:
          insecure_skip_verify: true
      - api_server: https://master:6443
        role: node
        namespaces:
          names: ["default"]
        bearer_token_file: /Users/ollie/opencode/prometheus/documentation/examples/k8s.token
        tls_config:
          insecure_skip_verify: true
    bearer_token_file:   /Users/ollie/opencode/prometheus/documentation/examples/k8s.token
    tls_config:
      insecure_skip_verify: true
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
```

源码debug示意图  

![discovery debug](src/discovery_debug.png "discovery debug")  


### 更新协程`updater goroutine`

通过`up chan<- []*targetgroup.Group`,`updater`协程获取到服务发现的结果信息。`updater`协程将服务发现的结果更新到`Discovery Manager`实例的`targets`字段。并且通过`triggerSend`(类型`chan struct{}`)向`sender`协程发送信号，通知服务发现有新更新。
`targets`字段的类型`map[poolKey]map[string]*targetgroup.Group` : 

- `poolKey` 信息包含`job_name`和`服务发现类型`和`序号`。poolKey定义(以配置demo1为例)
  
```golang
type poolKey struct {
	setName  string    // job_name  例如:kubernetes-nodes-jobnanme
	provider string    // 服务类型和序号构成的string，例如:kubernetes/1  标明此服务发现是kubernetes协议的。prometheus有很多kubernetes服务发现，顺序上此为第1个
}
```

- `map[string]*targetgroup.Group` 每个`target`的具体地址等信息

### 发送协程`sender goroutine`

`sender`协程接受到`triggerSend`(类型`chan struct{}`)发来的信息，得知服务发现有新的更新。`sender`协程在`Discovery Manager`实例的`targets`字段里获取**全部**的服务发现信息。将服务发现的结果通过`syncCh chan map[string][]*targetgroup.Group`发送给`scrape`模块。


## 源码分析



### 服务发现运行

![discovery core logic](src/服务发现逻辑.drawio.png "discovery core logic")  
<br/>  

<!--
配置文件解析     》  获取target 
               》  暂存target 
			   》  Discovery Manager  run   》 scrape
  -->

## 配置文件解析

在配置文件[`prometheus.yaml`](https://github.com/prometheus/prometheus/blob/main/documentation/examples/prometheus.yml)中`scrape_configs.xxxx_sd_configs`(例如:`kubernetes_sd_config`、`file_sd_config`)或者 `scrape_configs.static_config` 就是关于服务发现的配置。  




todo


## `Discoverer`实际执行者

`prometheus` 定义了 `Discoverer` 接口(*定义文件：`discovery/discovery.go`*)。`prometheus`中。 `Discoverer` 接口只有一个方法` Run(ctx context.Context, up chan<- []*targetgroup.Group) `。 `targets `变化都可以通过监听 `up chan` 获取到。我们将以`kubernetes`为例进行说明。  

**`Discoverer` 接口定义**:  
```go
// Discoverer provides information about target groups. It maintains a set
// of sources from which TargetGroups can originate. Whenever a discovery provider
// detects a potential change, it sends the TargetGroup through its channel.
//
// Discoverer does not know if an actual change happened.
// It does guarantee that it sends the new TargetGroup whenever a change happens.
//
// Discoverers should initially send a full set of all discoverable TargetGroups.

type Discoverer interface {
	// Run hands a channel to the discovery provider (Consul, DNS, etc.) through which
	// it can send updated target groups. It must return when the context is canceled.
	// It should not close the update channel on returning.
	Run(ctx context.Context, up chan<- []*targetgroup.Group)
}

```

## 新老版本Discovery Manager
todo
